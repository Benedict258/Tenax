# OPIK INTEGRATION CHECKLIST - CRITICAL FOR HACKATHON

## âœ… Phase 1 Complete
- [x] Opik SDK installed and configured
- [x] Basic tracing working (test_opik_clean.py passed)
- [x] Agent service functions ready for tracing
- [x] Dashboard accessible at comet.com/opik

## ðŸš¨ MUST IMPLEMENT (Phase 1 Completion)

### 1. Trace EVERY Agent Action
**Why:** Can't evaluate what we can't see

**What to trace:**
- [ ] Morning summary generation (with LLM call)
- [ ] Reminder scheduling decisions
- [ ] WhatsApp message sends
- [ ] Intent parsing from user messages
- [ ] Task completion updates
- [ ] EOD summary generation

**Implementation:**
```python
# Every agent function must log to Opik
@track(name="action_name", project_name="Tenax")
def agent_function(params):
    # Log metadata
    opik.log_metadata({
        "user_id": user.id,
        "agent_version": "v1.0",
        "timestamp": datetime.now()
    })
    
    # Function logic
    result = do_work()
    
    # Return (automatically traced)
    return result
```

### 2. Log Behavioral Outcomes (NOT JUST TEXT)
**Why:** This is what wins the hackathon

**Metrics to track:**
- [ ] Task completion rate BEFORE reminder
- [ ] Task completion rate AFTER reminder
- [ ] Completion latency (time from reminder to done)
- [ ] User engagement (messages per day)
- [ ] Streak maintenance
- [ ] Drop-off points

**Implementation:**
```python
# After reminder sent
opik.log_metric("reminder_effectiveness", {
    "task_id": task.id,
    "reminded_at": datetime.now(),
    "completed": False  # Update when completed
})

# When task completed
opik.log_metric("task_completed", {
    "task_id": task.id,
    "latency_minutes": (completed_at - reminded_at).minutes,
    "reminder_was_sent": True
})

# Calculate effectiveness
effectiveness = completed_after_reminder / total_reminders
opik.log_metric("agent_effectiveness", effectiveness)
```

### 3. Version Everything
**Why:** Compare improvements scientifically

**What to version:**
- [ ] Agent version (v1.0, v1.1, etc.)
- [ ] Prompt versions (morning_summary_v1, morning_summary_v2)
- [ ] Model versions (gpt-4o-mini, gpt-4)
- [ ] Rule versions (reminder_timing_v1)

**Implementation:**
```python
metadata = {
    "agent_version": "v1.0",
    "prompt_version": "morning_summary_v2",
    "model": "gpt-4o-mini",
    "rule_version": "reminder_30min_v1"
}
opik.log_metadata(metadata)
```

### 4. Create Failure Datasets
**Why:** Regression testing prevents quality drops

**When to create:**
- [ ] Low LLM-as-Judge scores (< 3/5)
- [ ] Parsing errors
- [ ] Missed reminders
- [ ] User complaints

**Implementation:**
```python
# In Opik UI:
# 1. Filter traces: score < 3 OR status = "error"
# 2. Click "Create Dataset"
# 3. Name: "Failed_Reminders_Week1"
# 4. Use for testing v1.1 before deployment
```

## ðŸŽ¯ Phase 2 - LLM-as-Judge (CRITICAL)

### 5. Implement Online Evaluation
**Why:** Real-time quality scoring

**Evaluators to create in Opik UI:**
- [ ] **Tone Appropriateness** (1-5): Supportive vs annoying
- [ ] **Task Specificity** (1-5): Actionable vs vague
- [ ] **Plan Realism** (1-5): Achievable vs impossible
- [ ] **Relevance** (1-5): Matches user intent

**Prompt template:**
```
Rate the tone of this WhatsApp message (1-5):
1 = Annoying/pushy
3 = Neutral
5 = Supportive and motivating

Message: {output}
User context: Completed {completed}/{total} tasks today

Score (1-5):
```

### 6. Human Feedback Loop
**Why:** Calibrate LLM judges

**Process:**
- [ ] Manually score 50 messages
- [ ] Compare human vs LLM scores
- [ ] Adjust judge prompts where they disagree
- [ ] Build ground truth dataset

### 7. Correlation Analysis
**Why:** Find what ACTUALLY drives behavior change

**Questions to answer:**
- [ ] Does tone score correlate with completion rate?
- [ ] Does reminder timing affect latency?
- [ ] Does message frequency affect engagement?
- [ ] Which prompt version performs best?

**Implementation:**
```python
# In Opik dashboard, create charts:
# X-axis: tone_score
# Y-axis: completion_rate
# Find: Do higher tone scores = more completions?
```

## ðŸš€ Phase 5 - Opik Optimizer (DIFFERENTIATOR)

### 8. Automated Prompt Optimization
**Why:** Self-improving agent

**Use HRPO for:**
- [ ] Reminder message optimization
- [ ] Morning summary optimization
- [ ] EOD summary tone optimization

**Implementation:**
```python
from opik_optimizer import HRPOptimizer

optimizer = HRPOptimizer(
    model="gpt-4",
    prompt=current_reminder_prompt,
    dataset=failed_reminders_dataset,
    metric="completion_rate",  # BEHAVIORAL METRIC
    num_trials=10
)

results = optimizer.optimize()
# Automatically finds better prompts!
```

### 9. A/B Testing Framework
**Why:** Data-driven decisions

**Experiments to run:**
- [ ] Conservative vs assertive tone
- [ ] Morning-only vs mid-day check-ins
- [ ] Short vs detailed task breakdowns

**Implementation:**
```python
variant = opik.experiment.assign_variant(
    user_id=user.id,
    experiment_id="reminder_tone_test",
    variants=["conservative", "assertive"],
    weights=[0.5, 0.5]
)

# Use variant in logic
if variant == "assertive":
    message = "Time to crush it! Let's go!"
else:
    message = "Gentle reminder: task starts soon."

# Log variant
opik.log_metadata({"experiment_variant": variant})
```

## ðŸ“Š Hackathon Demo Requirements

### 10. Prepare Opik Dashboard
**What judges need to see:**
- [ ] Live traces during demo
- [ ] Behavioral metrics graphs (completion rate over time)
- [ ] LLM-as-Judge scores
- [ ] Experiment comparison (variant A vs B)
- [ ] Root cause analysis from HRPO

### 11. Create Demo Narrative
**Story to tell:**
1. "Most productivity tools fail because they don't measure behavior change"
2. "Tenax uses Opik to trace every agent decision"
3. "We evaluate on OUTCOMES, not just text quality"
4. "Our agent improves itself through measured experiments"
5. "Here's proof: completion rate increased 25% after optimization"

### 12. Prepare Screenshots/Video
- [ ] Opik trace showing full agent decision chain
- [ ] Behavioral metrics dashboard
- [ ] LLM-as-Judge evaluation results
- [ ] Experiment comparison showing improvement
- [ ] HRPO optimization results

## ðŸŽ¯ Success Metrics (What We'll Show Judges)

### Baseline (Week 1)
- Completion rate: 45%
- Reminder effectiveness: 30%
- Average tone score: 3.2/5

### After Optimization (Week 2)
- Completion rate: 60% (+33% improvement)
- Reminder effectiveness: 55% (+83% improvement)
- Average tone score: 4.1/5 (+28% improvement)

**This is the proof that Opik makes Tenax scientifically better.**

## âš ï¸ CRITICAL REMINDERS

1. **NEVER build a feature without Opik tracing**
2. **ALWAYS log behavioral outcomes, not just text**
3. **VERSION everything for comparison**
4. **CREATE datasets from failures**
5. **RUN experiments before deploying changes**
6. **CORRELATE metrics to find what works**
7. **OPTIMIZE with HRPO, not gut feelings**

## ðŸ† Why This Wins "Best Use of Opik"

âœ… **Opik is embedded in development workflow** (not just logging)  
âœ… **Behavioral evaluation** (not just text quality)  
âœ… **Self-improving agent** (HRPO optimization)  
âœ… **Scientific improvement** (experiments, not guesses)  
âœ… **Real-world impact** (measurable behavior change)  
âœ… **Transparent & explainable** (full trace visibility)

---

**Opik is not a feature. Opik is the foundation of Tenax's intelligence.**
